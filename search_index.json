[["index.html", "Darwin Core Guide Preface", " Darwin Core Guide Standardizing Marine Biological Data Working Group 2022-02-17 Preface Biological data structures, definitions, measurements, and linkages are neccessarily as diverse as the systems they represent. This presents a real challenge when integrating data across biological research domains such as ecology, oceanography, fisheries, and climate sciences. Lots of standards exist for use with biological data but navigating them can be difficult for data managers who are new to them. The Earth Science Information Partners (ESIP) Biological Data Standards Cluster developed this primer for managers of biological data to provide a quick, easy resource for navigating a selection of the standards that exist. The goal of the primer is to spread awareness about existing standards and is intended to be shared online and at conferences to increase the adoption of standards for biological data and make them FAIR. Benson, Abigail; LaScala-Gruenewald, Diana; McGuinn, Robert; Satterthwaite, Erin; Beaulieu, Stace; Biddle, Mathew; et al. (2021): Biological Observation Data Standardization - A Primer for Data Managers. ESIP. Online resource. https://doi.org/10.6084/m9.figshare.16806712.v1 "],["intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction The world of standardizing marine biological data can seem complex for the naive oceanographer, biologist, scientist, or programmer. Transforming and integrating data is about combining the right standards for your desired interoperability with other data types. For example, interoperating fish biology measurements with climate level variables. There are a few concepts necessary to make this possible such as standard data structures, controlled vocabularies and knowledge representations, along with metadata standards to facilitate data discovery. This will permit the inclusion of more data and broader access to better ecosystem based models. Many scientific domains data handling practices are currently being reshaped in light of recent advances in computing power, technology, and data science. "],["applications.html", "Chapter 2 Applications 2.1 Salmon Ocean Ecology Data 2.2 Hakai Seagrass 2.3 Trawl Data 2.4 Aligning Data to Darwin Core - Sampling Event with Measurement or Fact", " Chapter 2 Applications Some applications are demonstrated in this chapter. 2.1 Salmon Ocean Ecology Data 2.1.1 Intro One of the goals of the Hakai Institute and the Canadian Integrated Ocean Observing System (CIOOS) is to facilitate Open Science and FAIR (findable, accessible, interoperable, reusable) ecological and oceanographic data. In a concerted effort to adopt or establish how best to do that, several Hakai and CIOOS staff attended an International Ocean Observing System (IOOS) Code Sprint in Ann Arbour, Michigan between October 7–11, 2019, to discuss how to implement FAIR data principles for biological data collected in the marine environment. The Darwin Core is a highly structured data format that standardizes data table relations, vocabularies, and defines field names. The Darwin Core defines three table types: event, occurrence, and measurementOrFact. This intuitively captures the way most ecologists conduct their research. Typically, a survey (event) is conducted and measurements, counts, or observations (collectively measurementOrFacts) are made regarding a specific habitat or species (occurrence). In the following script I demonstrate how I go about converting a subset of the data collected from the Hakai Institute Juvenile Salmon Program and discuss challenges, solutions, pros and cons, and when and what’s worthwhile to convert to Darwin Core. The conversion of a dataset to Darwin Core is much easier if your data are already tidy (normalized) in which you represent your data in separate tables that reflect the hierarchical and related nature of your observations. If your data are not already in a consistent and structured format, the conversion would likely be very arduos and not intuitive. 2.1.2 event The first step is to consider what you will define as an event in your data set. I defined the capture of fish using a purse seine net as the event. Therefore, each row in the event table is one deployment of a seine net and is assigned a unique eventID. My process for conversion was to make a new table called event and map the standard Darwin Core column names to pre-existing columns that serve the same purpose in my original seine_data table and populate the other required fields. event &lt;- tibble(eventID = survey_seines$seine_id, eventDate = date(survey_seines$survey_date), decimalLatitude = survey_seines$lat, decimalLongitude = survey_seines$long, geodeticDatum = &quot;EPSG:4326 WGS84&quot;, minimumDepthInMeters = 0, maximumDepthInMeters = 9, # seine depth is 9 m samplingProtocol = &quot;http://dx.doi.org/10.21966/1.566666&quot; # This is the DOI for the Hakai Salmon Data Package that contains the smnpling protocol, as well as the complete data package ) write_csv(event, here::here(&quot;datasets&quot;, &quot;hakai_salmon_data&quot;, &quot;event.csv&quot;)) 2.1.3 occurrence Next you’ll want to determine what constitutes an occurrence for your data set. Because each event caputers fish, I consider each fish to be an occurrence. Therefore, the unit of observation (each row) in the occurrence table is a fish. To link each occurence to an event you need to include the eventID column for every occurrence so that you know what seine (event) each fish (occurrence) came from. You must also provide a globally unique identifier for each occurrence. I already have a locally unique identifier for each fish in the original fish_data table called ufn. To make it globally unique I pre-pend the organization and research program metadata to the ufn column. #TODO: Include bycatch data as well ## make table long first seines_total_long &lt;- survey_seines %&gt;% select(seine_id, so_total, pi_total, cu_total, co_total, he_total, ck_total) %&gt;% pivot_longer(-seine_id, names_to = &quot;scientificName&quot;, values_to = &quot;n&quot;) seines_total_long$scientificName &lt;- recode(seines_total_long$scientificName, so_total = &quot;Oncorhynchus nerka&quot;, pi_total = &quot;Oncorhynchus gorbushca&quot;, cu_total = &quot;Oncorhynchus keta&quot;, co_total = &quot;Oncorhynchus kisutch&quot;, ck_total = &quot;Oncorhynchus tshawytscha&quot;, he_total = &quot;Clupea pallasii&quot;) seines_taken_long &lt;- survey_seines %&gt;% select(seine_id, so_taken, pi_taken, cu_taken, co_taken, he_taken, ck_taken) %&gt;% pivot_longer(-seine_id, names_to = &quot;scientificName&quot;, values_to = &quot;n_taken&quot;) seines_taken_long$scientificName &lt;- recode(seines_taken_long$scientificName, so_taken = &quot;Oncorhynchus nerka&quot;, pi_taken = &quot;Oncorhynchus gorbushca&quot;, cu_taken = &quot;Oncorhynchus keta&quot;, co_taken = &quot;Oncorhynchus kisutch&quot;, ck_taken = &quot;Oncorhynchus tshawytscha&quot;, he_taken = &quot;Clupea pallasii&quot;) ## remove records that have already been assigned an ID seines_long &lt;- full_join(seines_total_long, seines_taken_long, by = c(&quot;seine_id&quot;, &quot;scientificName&quot;)) %&gt;% drop_na() %&gt;% mutate(n_not_taken = n - n_taken) %&gt;% #so_total includes the number taken so I subtract n_taken to get n_not_taken select(-n_taken, -n) %&gt;% filter(n_not_taken &gt; 0) all_fish_caught &lt;- seines_long[rep(seq.int(1, nrow(seines_long)), seines_long$n_not_taken), 1:3] %&gt;% select(-n_not_taken) %&gt;% mutate(prefix = &quot;hakai-jsp-&quot;, suffix = 1:nrow(.), occurrenceID = paste0(prefix, suffix) ) %&gt;% select(-prefix, -suffix) # # Change species names to full Scientific names latin &lt;- fct_recode(fish_data$species, &quot;Oncorhynchus nerka&quot; = &quot;SO&quot;, &quot;Oncorhynchus gorbuscha&quot; = &quot;PI&quot;, &quot;Oncorhynchus keta&quot; = &quot;CU&quot;, &quot;Oncorhynchus kisutch&quot; = &quot;CO&quot;, &quot;Clupea pallasii&quot; = &quot;HE&quot;, &quot;Oncorhynchus tshawytscha&quot; = &quot;CK&quot;) %&gt;% as.character() fish_retained_data &lt;- fish_data %&gt;% mutate(scientificName = latin) %&gt;% select(-species) %&gt;% mutate(prefix = &quot;hakai-jsp-&quot;, occurrenceID = paste0(prefix, ufn)) %&gt;% select(-semsp_id, -prefix, -ufn, -fork_length_field, -fork_length, -weight, -weight_field) occurrence &lt;- bind_rows(all_fish_caught, fish_retained_data) %&gt;% mutate(basisOfRecord = &quot;HumanObservation&quot;, occurenceStatus = &quot;present&quot;) %&gt;% rename(eventID = seine_id) For each occuerence of the six different fish species that I caught I need to match the species name that I provide with the official scientificName that is part of the World Register of Marine Species database http://www.marinespecies.org/ # I went directly to the WoRMS webite (http://www.marinespecies.org/) to download the full taxonomic levels for the salmon species I have and put the WoRMS output (species_matched.xls) table in this project directory which is read in below and joined with the occurrence table species_matched &lt;- readxl::read_excel(here::here(&quot;datasets&quot;, &quot;hakai_salmon_data&quot;, &quot;raw_data&quot;, &quot;species_matched.xls&quot;)) occurrence &lt;- left_join(occurrence, species_matched, by = c(&quot;scientificName&quot; = &quot;ScientificName&quot;)) %&gt;% select(occurrenceID, basisOfRecord, scientificName, eventID, occurrenceStatus = occurenceStatus, Kingdom, Phylum, Class, Order, Family, Genus, Species) write_csv(occurrence, here::here(&quot;datasets&quot;, &quot;hakai_salmon_data&quot;, &quot;occurrence.csv&quot;)) 2.1.4 measurementOrFact To convert all your measurements or facts from your normal format to Darwin Core you essentially need to put all your measurements into one column called measurementType and a corresponding column called MeasurementValue. This standardizes the column names are in the measurementOrFact table. There are a number of predefined measurementTypes listed on the NERC database that should be used where possible. I found it difficult to navigate this page to find the correct measurementType. Here I convert length, and weight measurements that relate to an event and an occurrence and call those measurementTypes as length and weight. fish_data$weight &lt;- coalesce(fish_data$weight, fish_data$weight_field) fish_data$fork_length &lt;- coalesce(fish_data$fork_length, fish_data$fork_length_field) fish_length &lt;- fish_data %&gt;% mutate(occurrenceID = paste0(&quot;hakai-jsp-&quot;, ufn)) %&gt;% select(occurrenceID, eventID = seine_id, fork_length, weight) %&gt;% mutate(measurementType = &quot;fork length&quot;, measurementValue = fork_length) %&gt;% select(eventID, occurrenceID, measurementType, measurementValue) %&gt;% mutate(measurementUnit = &quot;millimeters&quot;, measurementUnitID = &quot;http://vocab.nerc.ac.uk/collection/P06/current/UXMM/&quot;) fish_weight &lt;- fish_data %&gt;% mutate(occurrenceID = paste0(&quot;hakai-jsp-&quot;, ufn)) %&gt;% select(occurrenceID, eventID = seine_id, fork_length, weight) %&gt;% mutate(measurementType = &quot;mass&quot;, measurementValue = weight) %&gt;% select(eventID, occurrenceID, measurementType, measurementValue) %&gt;% mutate(measurementUnit = &quot;grams&quot;, measurementUnitID = &quot;http://vocab.nerc.ac.uk/collection/P06/current/UGRM/&quot;) measurementOrFact &lt;- bind_rows(fish_length, fish_weight) %&gt;% drop_na(measurementValue) rm(fish_length, fish_weight) write_csv(measurementOrFact, here::here(&quot;datasets&quot;, &quot;hakai_salmon_data&quot;, &quot;measurementOrFact.csv&quot;)) 2.2 Hakai Seagrass 2.2.1 Setup This section clears the workspace, checks the working directory, and installs packages (if required) and loads packages, and loads necessary datasets library(&quot;knitr&quot;) # Knitr global chunk options opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE) 2.2.1.1 Load Data First load the seagrass density survey data, set variable classes, and have a quick look # Load density data seagrassDensity &lt;- read.csv(&quot;raw_data/seagrass_density_survey.csv&quot;, colClass = &quot;character&quot;) %&gt;% mutate(date = ymd(date), depth = as.numeric(depth), transect_dist = factor(transect_dist), collected_start = ymd_hms(collected_start), collected_end = ymd_hms(collected_end), density = as.numeric(density), density_msq = as.numeric(density_msq), canopy_height_cm = as.numeric(canopy_height_cm), flowering_shoots = as.numeric(flowering_shoots)) %T&gt;% glimpse() ## Rows: 3,031 ## Columns: 22 ## $ X &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;1… ## $ organization &lt;chr&gt; &quot;HAKAI&quot;, &quot;HAKAI&quot;, &quot;HAKAI&quot;, &quot;HAKAI&quot;, &quot;HAKAI&quot;, &quot;HAKAI&quot;,… ## $ work_area &lt;chr&gt; &quot;CALVERT&quot;, &quot;CALVERT&quot;, &quot;CALVERT&quot;, &quot;CALVERT&quot;, &quot;CALVERT&quot;… ## $ project &lt;chr&gt; &quot;MARINEGEO&quot;, &quot;MARINEGEO&quot;, &quot;MARINEGEO&quot;, &quot;MARINEGEO&quot;, &quot;… ## $ survey &lt;chr&gt; &quot;PRUTH_BAY&quot;, &quot;PRUTH_BAY&quot;, &quot;PRUTH_BAY&quot;, &quot;PRUTH_BAY&quot;, &quot;… ## $ site_id &lt;chr&gt; &quot;PRUTH_BAY_INTERIOR4&quot;, &quot;PRUTH_BAY_INTERIOR4&quot;, &quot;PRUTH_… ## $ date &lt;date&gt; 2016-05-13, 2016-05-13, 2016-05-13, 2016-05-13, 2016… ## $ sampling_bout &lt;chr&gt; &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;… ## $ dive_supervisor &lt;chr&gt; &quot;Zach&quot;, &quot;Zach&quot;, &quot;Zach&quot;, &quot;Zach&quot;, &quot;Zach&quot;, &quot;Zach&quot;, &quot;Zach… ## $ collector &lt;chr&gt; &quot;Derek&quot;, &quot;Derek&quot;, &quot;Derek&quot;, &quot;Derek&quot;, &quot;Derek&quot;, &quot;Derek&quot;,… ## $ hakai_id &lt;chr&gt; &quot;2016-05-13_PRUTH_BAY_INTERIOR4_0&quot;, &quot;2016-05-13_PRUTH… ## $ sample_type &lt;chr&gt; &quot;seagrass_density&quot;, &quot;seagrass_density&quot;, &quot;seagrass_den… ## $ depth &lt;dbl&gt; 6.0, 6.0, 6.0, 6.0, 5.0, 6.0, 6.0, 9.1, 9.0, 8.9, 9.0… ## $ transect_dist &lt;fct&gt; 0, 5, 10, 15, 20, 25, 30, 10, 15, 20, 25, 30, 0, 5, 1… ## $ collected_start &lt;dttm&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ collected_end &lt;dttm&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ density &lt;dbl&gt; 13, 10, 18, 22, 16, 31, 9, 5, 6, 6, 6, 3, 13, 30, 23,… ## $ density_msq &lt;dbl&gt; 208, 160, 288, 352, 256, 496, 144, 80, 96, 96, 96, 48… ## $ canopy_height_cm &lt;dbl&gt; 60, 63, 80, 54, 55, 50, 63, 85, 80, 90, 95, 75, 60, 6… ## $ flowering_shoots &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 0, 0, 0, 0, 0, NA, NA, NA… ## $ comments &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ quality_log &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… Next, load the habitat survey data, and same as above, set variable classes as necessary, and have a quick look. # load habitat data, set variable classes, have a quick look seagrassHabitat &lt;- read.csv(&quot;raw_data/seagrass_habitat_survey.csv&quot;, colClasses = &quot;character&quot;) %&gt;% mutate(date = ymd(date), depth = as.numeric(depth), hakai_id = str_pad(hakai_id, 5, pad = &quot;0&quot;), transect_dist = factor(transect_dist), collected_start = ymd_hms(collected_start), collected_end = ymd_hms(collected_end)) %T&gt;% glimpse() ## Rows: 2,052 ## Columns: 28 ## $ X &lt;chr&gt; &quot;1&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;7&quot;, &quot;8&quot;, &quot;9&quot;, &quot;10&quot;, &quot;1… ## $ organization &lt;chr&gt; &quot;HAKAI&quot;, &quot;HAKAI&quot;, &quot;HAKAI&quot;, &quot;HAKAI&quot;, &quot;HAKAI&quot;, &quot;HAKAI&quot;,… ## $ work_area &lt;chr&gt; &quot;CALVERT&quot;, &quot;CALVERT&quot;, &quot;CALVERT&quot;, &quot;CALVERT&quot;, &quot;CALVERT&quot;… ## $ project &lt;chr&gt; &quot;MARINEGEO&quot;, &quot;MARINEGEO&quot;, &quot;MARINEGEO&quot;, &quot;MARINEGEO&quot;, &quot;… ## $ survey &lt;chr&gt; &quot;CHOKED_PASS&quot;, &quot;CHOKED_PASS&quot;, &quot;CHOKED_PASS&quot;, &quot;CHOKED_… ## $ site_id &lt;chr&gt; &quot;CHOKED_PASS_INTERIOR6&quot;, &quot;CHOKED_PASS_INTERIOR6&quot;, &quot;CH… ## $ date &lt;date&gt; 2017-11-22, 2017-11-22, 2017-11-22, 2017-11-22, 2017… ## $ sampling_bout &lt;chr&gt; &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;… ## $ dive_supervisor &lt;chr&gt; &quot;gillian&quot;, &quot;gillian&quot;, &quot;gillian&quot;, &quot;gillian&quot;, &quot;gillian&quot;… ## $ collector &lt;chr&gt; &quot;zach&quot;, &quot;zach&quot;, &quot;zach&quot;, &quot;zach&quot;, &quot;zach&quot;, &quot;zach&quot;, &quot;kyle… ## $ hakai_id &lt;chr&gt; &quot;10883&quot;, &quot;2017-11-22_CHOKED_PASS_INTERIOR6_5 - 10&quot;, &quot;… ## $ sample_type &lt;chr&gt; &quot;seagrass_habitat&quot;, &quot;seagrass_habitat&quot;, &quot;seagrass_hab… ## $ depth &lt;dbl&gt; 9.2, 9.4, 9.3, 9.0, 9.2, 9.2, 3.4, 3.4, 3.4, 3.4, 3.4… ## $ transect_dist &lt;fct&gt; 0 - 5, 10-May, 15-Oct, 15 - 20, 20 - 25, 25 - 30, 0 -… ## $ collected_start &lt;dttm&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ collected_end &lt;dttm&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ bag_uid &lt;chr&gt; &quot;10883&quot;, NA, NA, &quot;11094&quot;, NA, &quot;11182&quot;, &quot;7119&quot;, NA, &quot;7… ## $ bag_number &lt;chr&gt; &quot;3557&quot;, NA, NA, &quot;3520&quot;, NA, &quot;903&quot;, &quot;800&quot;, NA, &quot;318&quot;, … ## $ density_range &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ substrate &lt;chr&gt; &quot;sand,shell hash&quot;, &quot;sand,shell hash&quot;, &quot;sand,shell has… ## $ patchiness &lt;chr&gt; &quot;&lt; 1&quot;, &quot;&lt; 1&quot;, &quot;02-Jan&quot;, &quot;&lt; 1&quot;, &quot;&lt; 1&quot;, &quot;&lt; 1&quot;, &quot;&lt; 1&quot;, &quot;… ## $ adj_habitat_1 &lt;chr&gt; &quot;seagrass&quot;, &quot;seagrass&quot;, &quot;seagrass&quot;, &quot;seagrass&quot;, &quot;seag… ## $ adj_habitat_2 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ sample_collected &lt;chr&gt; &quot;TRUE&quot;, &quot;FALSE&quot;, &quot;FALSE&quot;, &quot;TRUE&quot;, &quot;FALSE&quot;, &quot;TRUE&quot;, &quot;T… ## $ vegetation_1 &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;des&quot;, NA, &quot;des&quot;, NA, NA, NA,… ## $ vegetation_2 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ comments &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ quality_log &lt;chr&gt; &quot;1: Flowering shoots 0 for entire transects&quot;, NA, NA,… Finally, load coordinate data for surveys, and subset necessary variables coordinates &lt;- read.csv(&quot;raw_data/seagrassCoordinates.csv&quot;, colClass = c(&quot;Point.Name&quot; = &quot;character&quot;)) %&gt;% select(Point.Name, Decimal.Lat, Decimal.Long) %T&gt;% glimpse() ## Rows: 70 ## Columns: 3 ## $ Point.Name &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ Decimal.Lat &lt;dbl&gt; 52.06200, 52.05200, 51.92270, 51.92500, 51.80900, 51.8090… ## $ Decimal.Long &lt;dbl&gt; -128.4120, -128.4030, -128.4648, -128.4540, -128.2360, -1… 2.2.1.2 Merge Datasets Now all the datasets have been loaded, and briefly formatted, we’ll join together the habitat and density surveys, and the coordinates for these. The seagrass density surveys collect data at discrete points (ie. 5 metres) along the transects, while the habitat surveys collect data over sections (ie. 0 - 5 metres) along the transects. In order to fit these two surveys together, we’ll narrow the habitat surveys from a range to a point so the locations will match. Based on how the habitat data is collected, the point the habitat survey is applied to will be the distance at the end of the swath (ie. 10-15m will become 15m). To account for no preceeding distance, the 0m distance will use the 0-5m section of the survey. First, well make the necessary transformations to the habitat dataset. # Reformat seagrassHabitat to merge with seagrassDensity ## replicate 0 - 5m transect dist to match with 0m in density survey; ## rest of habitat bins can map one to one with density (ie. 5 - 10m -&gt; 10m) seagrass0tmp &lt;- seagrassHabitat %&gt;% filter(transect_dist %in% c(&quot;0 - 5&quot;, &quot;0 - 2.5&quot;)) %&gt;% mutate(transect_dist = factor(0)) ## collapse various levels to match with seagrassDensity transect_dist seagrassHabitat$transect_dist &lt;- fct_collapse(seagrassHabitat$transect_dist, &quot;5&quot; = c(&quot;0 - 5&quot;, &quot;2.5 - 7.5&quot;), &quot;10&quot; = c(&quot;5 - 10&quot;, &quot;7.5 - 12.5&quot;), &quot;15&quot; = c(&quot;10 - 15&quot;, &quot;12.5 - 17.5&quot;), &quot;20&quot; = c(&quot;15 - 20&quot;, &quot;17.5 - 22.5&quot;), &quot;25&quot; = c(&quot;20 - 25&quot;, &quot;22.5 - 27.5&quot;), &quot;30&quot; = c(&quot;25 - 30&quot;, &quot;27.5 - 30&quot;)) ## merge seagrass0tmp into seagrassHabitat to account for 0m samples, ## set class for date, datetime variables seagrassHabitatFull &lt;- rbind(seagrass0tmp, seagrassHabitat) %&gt;% filter(transect_dist != &quot;0 - 2.5&quot;) %&gt;% # already captured in seagrass0tmp droplevels(.) # remove now unused factor levels With the distances of habitat and density surveys now corresponding, we can now merge these two datasets plus there coordinates together, combine redundant fields, and remove unnecessary fields. # Merge seagrassHabitatFull with seagrassDensity, then coordinates seagrass &lt;- full_join(seagrassHabitatFull, seagrassDensity, by = c(&quot;organization&quot;, &quot;work_area&quot;, &quot;project&quot;, &quot;survey&quot;, &quot;site_id&quot;, &quot;date&quot;, &quot;transect_dist&quot;)) %&gt;% # merge hakai_id.x and hakai_id.y into single variable field; # use combination of date, site_id, transect_dist, and field uid (hakai_id # when present) mutate(field_uid = ifelse(sample_collected == TRUE, hakai_id.x, &quot;NA&quot;), hakai_id = paste(date, &quot;HAKAI:CALVERT&quot;, site_id, transect_dist, sep = &quot;:&quot;), # below, aggregate metadata that didn&#39;t merge naturally (ie. due to minor # differences in watch time or depth gauges) dive_supervisor = dive_supervisor.x, collected_start = ymd_hms(ifelse(is.na(collected_start.x), collected_start.y, collected_start.x)), collected_end = ymd_hms(ifelse(is.na(collected_start.x), collected_start.y, collected_start.x)), depth_m = ifelse(is.na(depth.x), depth.y, depth.x), sampling_bout = sampling_bout.x) %&gt;% left_join(., coordinates, # add coordinates by = c(&quot;site_id&quot; = &quot;Point.Name&quot;)) %&gt;% select( - c(X.x, X.y, hakai_id.x, hakai_id.y, # remove unnecessary variables dive_supervisor.x, dive_supervisor.y, collected_start.x, collected_start.y, collected_end.x, collected_end.y, depth.x, depth.y, sampling_bout.x, sampling_bout.y)) %&gt;% mutate(density_msq = as.character(density_msq), canopy_height_cm = as.character(canopy_height_cm), flowering_shoots = as.character(flowering_shoots), depth_m = as.character(depth_m)) %T&gt;% glimpse() ## Rows: 3,743 ## Columns: 38 ## $ organization &lt;chr&gt; &quot;HAKAI&quot;, &quot;HAKAI&quot;, &quot;HAKAI&quot;, &quot;HAKAI&quot;, &quot;HAKAI&quot;, &quot;HAKAI&quot;,… ## $ work_area &lt;chr&gt; &quot;CALVERT&quot;, &quot;CALVERT&quot;, &quot;CALVERT&quot;, &quot;CALVERT&quot;, &quot;CALVERT&quot;… ## $ project &lt;chr&gt; &quot;MARINEGEO&quot;, &quot;MARINEGEO&quot;, &quot;MARINEGEO&quot;, &quot;MARINEGEO&quot;, &quot;… ## $ survey &lt;chr&gt; &quot;CHOKED_PASS&quot;, &quot;CHOKED_PASS&quot;, &quot;CHOKED_PASS&quot;, &quot;PRUTH_B… ## $ site_id &lt;chr&gt; &quot;CHOKED_PASS_INTERIOR6&quot;, &quot;CHOKED_PASS_EDGE1&quot;, &quot;CHOKED… ## $ date &lt;date&gt; 2017-11-22, 2017-05-19, 2017-05-19, 2017-07-03, 2017… ## $ collector.x &lt;chr&gt; &quot;zach&quot;, &quot;kyle&quot;, NA, &quot;tanya&quot;, &quot;zach&quot;, &quot;zach&quot;, &quot;zach&quot;, … ## $ sample_type.x &lt;chr&gt; &quot;seagrass_habitat&quot;, &quot;seagrass_habitat&quot;, &quot;seagrass_hab… ## $ transect_dist &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ bag_uid &lt;chr&gt; &quot;10883&quot;, &quot;7119&quot;, &quot;7031&quot;, &quot;2352&quot;, &quot;10255&quot;, &quot;10023&quot;, &quot;1… ## $ bag_number &lt;chr&gt; &quot;3557&quot;, &quot;800&quot;, &quot;301&quot;, &quot;324&quot;, &quot;3506&quot;, &quot;3555&quot;, &quot;3534&quot;, … ## $ density_range &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ substrate &lt;chr&gt; &quot;sand,shell hash&quot;, &quot;sand,shell hash&quot;, &quot;sand,shell has… ## $ patchiness &lt;chr&gt; &quot;&lt; 1&quot;, &quot;&lt; 1&quot;, &quot;&lt; 1&quot;, &quot;&lt; 1&quot;, &quot;&lt; 1&quot;, &quot;05-Apr&quot;, &quot;04-Mar&quot;… ## $ adj_habitat_1 &lt;chr&gt; &quot;seagrass&quot;, &quot;sand&quot;, &quot;standing kelp&quot;, &quot;seagrass&quot;, &quot;sea… ## $ adj_habitat_2 &lt;chr&gt; NA, NA, NA, NA, NA, NA, &quot;standing kelp&quot;, NA, NA, NA, … ## $ sample_collected &lt;chr&gt; &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE… ## $ vegetation_1 &lt;chr&gt; NA, &quot;des&quot;, &quot;des&quot;, &quot;zm&quot;, &quot;des&quot;, NA, NA, NA, NA, NA, NA… ## $ vegetation_2 &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;… ## $ comments.x &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ quality_log.x &lt;chr&gt; &quot;1: Flowering shoots 0 for entire transects&quot;, NA, NA,… ## $ collector.y &lt;chr&gt; &quot;derek&quot;, &quot;ondine&quot;, &quot;ondine&quot;, &quot;derek&quot;, &quot;derek&quot;, &quot;derek… ## $ sample_type.y &lt;chr&gt; &quot;seagrass_density&quot;, &quot;seagrass_density&quot;, &quot;seagrass_den… ## $ density &lt;dbl&gt; 4, 10, 6, 13, 6, 1, 2, 6, 21, 3, 7, 4, 3, 14, 17, 11,… ## $ density_msq &lt;chr&gt; &quot;64&quot;, &quot;160&quot;, &quot;96&quot;, &quot;208&quot;, &quot;96&quot;, &quot;16&quot;, &quot;32&quot;, &quot;96&quot;, &quot;33… ## $ canopy_height_cm &lt;chr&gt; &quot;80&quot;, &quot;80&quot;, &quot;110&quot;, &quot;60&quot;, &quot;125&quot;, &quot;100&quot;, &quot;100&quot;, &quot;125&quot;, … ## $ flowering_shoots &lt;chr&gt; &quot;0&quot;, NA, NA, NA, NA, NA, NA, &quot;0&quot;, NA, NA, NA, &quot;0&quot;, NA… ## $ comments.y &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ quality_log.y &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, &quot;… ## $ field_uid &lt;chr&gt; &quot;10883&quot;, &quot;07119&quot;, &quot;07031&quot;, &quot;02352&quot;, &quot;10255&quot;, &quot;10023&quot;,… ## $ hakai_id &lt;chr&gt; &quot;2017-11-22:HAKAI:CALVERT:CHOKED_PASS_INTERIOR6:0&quot;, &quot;… ## $ dive_supervisor &lt;chr&gt; &quot;gillian&quot;, &quot;gillian,gillian.sadlierbrown&quot;, &quot;gillian,g… ## $ collected_start &lt;dttm&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ collected_end &lt;dttm&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ depth_m &lt;chr&gt; &quot;9.2&quot;, &quot;3.4&quot;, &quot;4.8&quot;, &quot;2.4&quot;, &quot;5.3&quot;, &quot;5.6&quot;, &quot;4.4&quot;, &quot;2.5… ## $ sampling_bout &lt;chr&gt; &quot;6&quot;, &quot;1&quot;, &quot;3&quot;, &quot;5&quot;, &quot;5&quot;, &quot;3&quot;, &quot;5&quot;, &quot;2&quot;, &quot;1&quot;, &quot;2&quot;, &quot;6&quot;… ## $ Decimal.Lat &lt;dbl&gt; 51.67482, 51.67882, 51.67493, 51.64532, 51.67349, 51.… ## $ Decimal.Long &lt;dbl&gt; -128.1195, -128.1148, -128.1237, -128.1193, -128.1180… 2.2.2 Convert Data to Darwin Core - Extended Measurement or Fact format The Darwin Core ExtendedMeasurementOrFact (eMoF) extension bases records around a core event (rather than occurrence as in standard Darwin Core), allowing for additional measurement variables to be associated with occurrence data. 2.2.2.1 Add Event ID and Occurrence ID variables to dataset As this dataset will be annually updated, rather than using natural keys (ie. using package::uuid to autogenerate) for event and occurence IDs, here we will use surrogate keys made up of a concatenation of date survey, transect location, observation distance, and sample ID (for occurrenceID, when a sample is present). # create and populate eventID variable ## currently only event is used, but additional surveys and abiotic data ## are associated with parent events that may be included at a later date seagrass$eventID &lt;- seagrass$hakai_id # create and populate occurrenceID; combine eventID with transect_dist # and field_uid ## in the event of &lt;NA&gt; field_uid, no sample was collected, but ## measurements and occurrence are still taken; no further subsamples ## are associated with &lt;NA&gt; field_uids seagrass$occurrenceID &lt;- with(seagrass, paste(eventID, transect_dist, field_uid, sep = &quot;:&quot;)) 2.2.2.2 Create Event, Occurrence, and eMoF tables Now that we’ve created eventIDs and occurrenceIDs to connect all the variables together, we can begin to create the Event, Occurrence, and extended Measurement or Fact table necessary for DarwinCore compliant datasets 2.2.2.2.1 Event Table # subset seagrass to create event table seagrassEvent &lt;- seagrass %&gt;% distinct %&gt;% # some duplicates in data stemming from database conflicts select(date, Decimal.Lat, Decimal.Long, transect_dist, depth_m, eventID) %&gt;% rename(eventDate = date, decimalLatitude = Decimal.Lat, decimalLongitude = Decimal.Long, coordinateUncertaintyInMeters = transect_dist, minimumDepthInMeters = depth_m, maximumDepthInMeters = depth_m) %&gt;% mutate(geodeticDatum = &quot;WGS84&quot;, samplingEffort = &quot;30 metre transect&quot;) %T&gt;% glimpse ## Rows: 3,659 ## Columns: 8 ## $ eventDate &lt;date&gt; 2017-11-22, 2017-05-19, 2017-05-19, 201… ## $ decimalLatitude &lt;dbl&gt; 51.67482, 51.67882, 51.67493, 51.64532, … ## $ decimalLongitude &lt;dbl&gt; -128.1195, -128.1148, -128.1237, -128.11… ## $ coordinateUncertaintyInMeters &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ maximumDepthInMeters &lt;chr&gt; &quot;9.2&quot;, &quot;3.4&quot;, &quot;4.8&quot;, &quot;2.4&quot;, &quot;5.3&quot;, &quot;5.6&quot;… ## $ eventID &lt;chr&gt; &quot;2017-11-22:HAKAI:CALVERT:CHOKED_PASS_IN… ## $ geodeticDatum &lt;chr&gt; &quot;WGS84&quot;, &quot;WGS84&quot;, &quot;WGS84&quot;, &quot;WGS84&quot;, &quot;WGS… ## $ samplingEffort &lt;chr&gt; &quot;30 metre transect&quot;, &quot;30 metre transect&quot;… # save event table to csv write.csv(seagrassEvent, &quot;processed_data/hakaiSeagrassDwcEvent.csv&quot;) 2.2.2.2.2 Occurrence Table # subset seagrass to create occurrence table seagrassOccurrence &lt;- seagrass %&gt;% distinct %&gt;% # some duplicates in data stemming from database conflicts select(eventID, occurrenceID) %&gt;% mutate(basisOfRecord = &quot;HumanObservation&quot;, scientificName = &quot;Zostera subg. Zostera marina&quot;, occurrenceStatus = &quot;present&quot;) # Taxonomic name matching # in addition to the above metadata, DarwinCore format requires further # taxonomic data that can be acquired through the WoRMS register. ## Load taxonomic info, downloaded via WoRMS tool # zmWorms &lt;- # read.delim(&quot;raw_data/zmworms_matched.txt&quot;, # header = TRUE, # nrows = 1) zmWorms &lt;- wm_record(id = 145795) # join WoRMS name with seagrassOccurrence create above seagrassOccurrence &lt;- full_join(seagrassOccurrence, zmWorms, by = c(&quot;scientificName&quot; = &quot;scientificname&quot;)) %&gt;% select(eventID, occurrenceID, basisOfRecord, scientificName, occurrenceStatus, AphiaID, url, authority, status, unacceptreason, taxonRankID, rank, valid_AphiaID, valid_name, valid_authority, parentNameUsageID, kingdom, phylum, class, order, family, genus, citation, lsid, isMarine, match_type, modified) %T&gt;% glimpse ## Rows: 3,659 ## Columns: 27 ## $ eventID &lt;chr&gt; &quot;2017-11-22:HAKAI:CALVERT:CHOKED_PASS_INTERIOR6:0&quot;, … ## $ occurrenceID &lt;chr&gt; &quot;2017-11-22:HAKAI:CALVERT:CHOKED_PASS_INTERIOR6:0:0:… ## $ basisOfRecord &lt;chr&gt; &quot;HumanObservation&quot;, &quot;HumanObservation&quot;, &quot;HumanObserv… ## $ scientificName &lt;chr&gt; &quot;Zostera subg. Zostera marina&quot;, &quot;Zostera subg. Zoste… ## $ occurrenceStatus &lt;chr&gt; &quot;present&quot;, &quot;present&quot;, &quot;present&quot;, &quot;present&quot;, &quot;present… ## $ AphiaID &lt;int&gt; 145795, 145795, 145795, 145795, 145795, 145795, 1457… ## $ url &lt;chr&gt; &quot;https://www.marinespecies.org/aphia.php?p=taxdetail… ## $ authority &lt;chr&gt; &quot;Linnaeus, 1753&quot;, &quot;Linnaeus, 1753&quot;, &quot;Linnaeus, 1753&quot;… ## $ status &lt;chr&gt; &quot;accepted&quot;, &quot;accepted&quot;, &quot;accepted&quot;, &quot;accepted&quot;, &quot;acc… ## $ unacceptreason &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ taxonRankID &lt;int&gt; 220, 220, 220, 220, 220, 220, 220, 220, 220, 220, 22… ## $ rank &lt;chr&gt; &quot;Species&quot;, &quot;Species&quot;, &quot;Species&quot;, &quot;Species&quot;, &quot;Species… ## $ valid_AphiaID &lt;int&gt; 145795, 145795, 145795, 145795, 145795, 145795, 1457… ## $ valid_name &lt;chr&gt; &quot;Zostera subg. Zostera marina&quot;, &quot;Zostera subg. Zoste… ## $ valid_authority &lt;chr&gt; &quot;Linnaeus, 1753&quot;, &quot;Linnaeus, 1753&quot;, &quot;Linnaeus, 1753&quot;… ## $ parentNameUsageID &lt;int&gt; 370435, 370435, 370435, 370435, 370435, 370435, 3704… ## $ kingdom &lt;chr&gt; &quot;Plantae&quot;, &quot;Plantae&quot;, &quot;Plantae&quot;, &quot;Plantae&quot;, &quot;Plantae… ## $ phylum &lt;chr&gt; &quot;Tracheophyta&quot;, &quot;Tracheophyta&quot;, &quot;Tracheophyta&quot;, &quot;Tra… ## $ class &lt;chr&gt; &quot;Magnoliopsida&quot;, &quot;Magnoliopsida&quot;, &quot;Magnoliopsida&quot;, &quot;… ## $ order &lt;chr&gt; &quot;Alismatales&quot;, &quot;Alismatales&quot;, &quot;Alismatales&quot;, &quot;Alisma… ## $ family &lt;chr&gt; &quot;Zosteraceae&quot;, &quot;Zosteraceae&quot;, &quot;Zosteraceae&quot;, &quot;Zoster… ## $ genus &lt;chr&gt; &quot;Zostera&quot;, &quot;Zostera&quot;, &quot;Zostera&quot;, &quot;Zostera&quot;, &quot;Zostera… ## $ citation &lt;chr&gt; &quot;WoRMS (2022). Zostera subg. Zostera marina Linnaeus… ## $ lsid &lt;chr&gt; &quot;urn:lsid:marinespecies.org:taxname:145795&quot;, &quot;urn:ls… ## $ isMarine &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ match_type &lt;chr&gt; &quot;exact&quot;, &quot;exact&quot;, &quot;exact&quot;, &quot;exact&quot;, &quot;exact&quot;, &quot;exact&quot;… ## $ modified &lt;chr&gt; &quot;2008-12-09T10:03:16.140Z&quot;, &quot;2008-12-09T10:03:16.140… # save occurrence table to csv write.csv(seagrassOccurrence, &quot;processed_data/hakaiSeagrassDwcOccurrence.csv&quot;) 2.2.2.2.3 Extended MeasurementOrFact table seagrassMof &lt;- seagrass %&gt;% # select variables for eMoF table select(date, eventID, survey, site_id, transect_dist, substrate, patchiness, adj_habitat_1, adj_habitat_2, vegetation_1, vegetation_2, density_msq, canopy_height_cm, flowering_shoots) %&gt;% # split substrate into two variables (currently holds two substrate type in same variable) separate(substrate, sep = &quot;,&quot;, into = c(&quot;substrate_1&quot;, &quot;substrate_2&quot;)) %&gt;% # change variables names to match NERC database (or to be more descriptive where none exist) rename(measurementDeterminedDate = date, SubstrateTypeA = substrate_1, SubstrateTypeB = substrate_2, BarePatchLengthWithinSeagrass = patchiness, PrimaryAdjacentHabitat = adj_habitat_1, SecondaryAdjacentHabitat = adj_habitat_2, PrimaryAlgaeSp = vegetation_1, SecondaryAlgaeSp = vegetation_2, BedAbund = density_msq, CanopyHeight = canopy_height_cm, FloweringBedAbund = flowering_shoots) %&gt;% # reformat variables into DwC MeasurementOrFact format # (single values variable, with measurement type, unit, etc. variables) pivot_longer( - c(measurementDeterminedDate, eventID, survey, site_id, transect_dist), names_to = &quot;measurementType&quot;, values_to = &quot;measurementValue&quot;, values_ptypes = list(measurementValue = &quot;character&quot;)) %&gt;% # use measurement type to fill in remainder of variables relating to # NERC vocabulary and metadata fields mutate( measurementTypeID = case_when( measurementType == &quot;BedAbund&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/P01/current/SDBIOL02/&quot;, measurementType == &quot;CanopyHeight&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/P01/current/OBSMAXLX/&quot;, # measurementType == &quot;BarePatchWithinSeagrass&quot; ~ &quot;&quot;, measurementType == &quot;FloweringBedAbund&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/P01/current/SDBIOL02/&quot;), measurementUnit = case_when( measurementType == &quot;BedAbund&quot; ~ &quot;Number per square metre&quot;, measurementType == &quot;CanopyHeight&quot; ~ &quot;Centimetres&quot;, measurementType == &quot;BarePatchhLengthWithinSeagrass&quot; ~ &quot;Metres&quot;, measurementType == &quot;FloweringBedAbund&quot; ~ &quot;Number per square metre&quot;), measurementUnitID = case_when( measurementType == &quot;BedAbund&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/P06/current/UPMS/&quot;, measurementType == &quot;CanopyHeight&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/P06/current/ULCM/&quot;, measurementType == &quot;BarePatchhLengthWithinSeagrass&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/P06/current/ULAA/2/&quot;, measurementType == &quot;FloweringBedAbund&quot; ~ &quot;http://vocab.nerc.ac.uk/collection/P06/current/UPMS/&quot;), measurementAccuracy = case_when( measurementType == &quot;CanopyHeight&quot; ~ 5), measurementMethod = case_when( measurementType == &quot;BedAbund&quot; ~ &quot;25cmx25cm quadrat count&quot;, measurementType == &quot;CanopyHeight&quot; ~ &quot;in situ with ruler&quot;, measurementType == &quot;BarePatchhLengthWithinSeagrass&quot; ~ &quot;estimated along transect line&quot;, measurementType == &quot;FloweringBedAbund&quot; ~ &quot;25cmx25cm quadrat count&quot;)) %&gt;% select(eventID, measurementDeterminedDate, measurementType, measurementValue, measurementTypeID, measurementUnit, measurementUnitID, measurementAccuracy, measurementMethod) %T&gt;% # select(!c(survey, site_id, transect_dist)) %T&gt;% glimpse() ## Rows: 37,430 ## Columns: 9 ## $ eventID &lt;chr&gt; &quot;2017-11-22:HAKAI:CALVERT:CHOKED_PASS_INTERI… ## $ measurementDeterminedDate &lt;date&gt; 2017-11-22, 2017-11-22, 2017-11-22, 2017-11… ## $ measurementType &lt;chr&gt; &quot;SubstrateTypeA&quot;, &quot;SubstrateTypeB&quot;, &quot;BarePat… ## $ measurementValue &lt;chr&gt; &quot;sand&quot;, &quot;shell hash&quot;, &quot;&lt; 1&quot;, &quot;seagrass&quot;, NA,… ## $ measurementTypeID &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, &quot;http://vocab.ne… ## $ measurementUnit &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, &quot;Number per squa… ## $ measurementUnitID &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, &quot;http://vocab.ne… ## $ measurementAccuracy &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 5, NA, NA, N… ## $ measurementMethod &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, &quot;25cmx25cm quadr… # save eMoF table to csv write.csv(seagrassMof, &quot;processed_data/hakaiSeagrassDwcEmof.csv&quot;) 2.2.3 Session Info Print session information below in case necessary for future reference # Print Session Info for future reference sessionInfo() ## R version 4.1.1 (2021-08-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur 10.16 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] worrms_0.4.2 magrittr_2.0.2 knitr_1.37 lubridate_1.8.0 ## [5] here_1.0.1 forcats_0.5.1 stringr_1.4.0 dplyr_1.0.8 ## [9] purrr_0.3.4 readr_2.1.2 tidyr_1.2.0 tibble_3.1.6 ## [13] ggplot2_3.3.5 tidyverse_1.3.1 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_1.0.8 assertthat_0.2.1 rprojroot_2.0.2 digest_0.6.29 ## [5] utf8_1.2.2 R6_2.5.1 cellranger_1.1.0 backports_1.4.1 ## [9] reprex_2.0.1 evaluate_0.14 httr_1.4.2 pillar_1.7.0 ## [13] rlang_1.0.1 curl_4.3.2 readxl_1.3.1 rstudioapi_0.13 ## [17] jquerylib_0.1.4 rmarkdown_2.11 urltools_1.7.3 triebeard_0.3.0 ## [21] bit_4.0.4 munsell_0.5.0 broom_0.7.12 compiler_4.1.1 ## [25] modelr_0.1.8 xfun_0.29 pkgconfig_2.0.3 htmltools_0.5.2 ## [29] tidyselect_1.1.1 httpcode_0.3.0 bookdown_0.24 fansi_1.0.2 ## [33] crayon_1.5.0 tzdb_0.2.0 dbplyr_2.1.1 withr_2.4.3 ## [37] crul_1.2.0 grid_4.1.1 jsonlite_1.7.3 gtable_0.3.0 ## [41] lifecycle_1.0.1 DBI_1.1.2 scales_1.1.1 cli_3.2.0 ## [45] stringi_1.7.6 vroom_1.5.7 fs_1.5.2 xml2_1.3.3 ## [49] ellipsis_0.3.2 generics_0.1.2 vctrs_0.3.8 tools_4.1.1 ## [53] bit64_4.0.5 glue_1.6.1 hms_1.1.1 parallel_4.1.1 ## [57] fastmap_1.1.0 yaml_2.2.2 colorspace_2.0-2 rvest_1.0.2 ## [61] haven_2.4.3 2.3 Trawl Data One of the more common datasets that can be standardized to Darwin Core and integrated within OBIS is catch data from e.g. a trawl sampling event, or a zooplankton net tow. Of special concern here are datasets that include both a total (species-specific) catch weight, in addition to individual measurements (for a subset of the overall data). In this case, through our standardization to Darwin Core, we want to ensure that data users understand that the individual measurements are a part of, or subset of, the overall (species-specific) record, whilst at the same time ensure that data providers are not duplicating occurrence records to OBIS. The GitHub issue related to application is can be found here 2.3.1 Workflow Overview In our current setup, this relationship between the overall catch data and subsetted information is provided in the resourceRelationship extension. This extension cannot currently be harvested by GBIF. The required terms for this extension are resourceID, relatedResourceID, resourceRelationshipID and relationshipOfResource. The relatedResourceID here refers to the object of the relationship, whereas the resourceID refers to the subject of the relationship: resourceRelationshipID: a unique identifier for the relationship between one resource (the subject) and another (relatedResource, object). resourceID: a unique identifier for the resource that is the subject of the relationship. relatedResourceID: a unique identifier for the resource that is the object of the relationship. relationshipOfResource: The relationship of the subject (identified by the resourceID) to the object (relatedResourceID). The relationshipOfResource is a free text field. A few resources have been published to OBIS that contain the resourceRelationship extension (examples). Here, I’ll lay out the process and coding used for the Trawl Catch and Species Abundance from the 2019 Gulf of Alaska International Year of the Salmon Expedition. In the following code chunks some details are omitted to improve the readability - the overall code to standardize the catch data can be found here. This dataset includes species-specific total catch data at multiple stations (sampling events). From each catch, individual measurements were also taken. Depending on the number of individual caught in the trawl, this was either the total number of species individuals caught, or only a subset (in case of large numbers of individuals caught). In this specific data record, we created a single Event Core with three extensions: an occurrence extension, measurement or fact extension, and the resourceRelationship extension. However, in this walk-through I’ll only touch on the Event Core, occurrence extension and resourceRelationship extension. The trawl data is part of a larger project collecting various data types related to salmon ocean ecology. Therefore, in our Event Core we nested information related to the sampling event in the specific layer. (include a visual representation of the schema). Prior to creating the Event Core, we ensured that e.g. dates and times followed the correct ISO-8601 standards, and converted to the correct time zone. # Time is recorded numerically (1037 instead of 10:37), so need to change these columns: trawl2019$END_DEPLOYMENT_TIME &lt;- substr(as.POSIXct(sprintf(&quot;%04.0f&quot;, trawl2019$END_DEPLOYMENT_TIME), format = &quot;%H%M&quot;), 12, 16) trawl2019$BEGIN_RETRIEVAL_TIME &lt;- substr(as.POSIXct(sprintf(&quot;%04.0f&quot;, trawl2019$BEGIN_RETRIEVAL_TIME), format = &quot;%H%M&quot;), 12, 16) # Additionally, the vessel time is recorded in &#39;Vladivostok&#39; according to the metadata tab. This has to be converted to UTC. trawl2019 &lt;- trawl2019 %&gt;% mutate(eventDate_start = format_iso_8601(as.POSIXct(paste(EVENT_DATE_START, END_DEPLOYMENT_TIME), tz = &quot;Asia/Vladivostok&quot;)), eventDate_start = str_replace(eventDate_start, &quot;\\\\+00:00&quot;, &quot;Z&quot;), eventDate_finish = format_iso_8601(as.POSIXct(paste(EVENT_DATE_FINISH, BEGIN_RETRIEVAL_TIME), tz = &quot;Asia/Vladivostok&quot;)), eventDate_finish = str_replace(eventDate_finish, &quot;\\\\+00:00&quot;, &quot;Z&quot;), eventDate = paste(eventDate_start, eventDate_finish, sep = &quot;/&quot;), project = &quot;IYS&quot;, cruise = paste(project, &quot;GoA2019&quot;, sep = &quot;:&quot;), station = paste(cruise, TOW_NUMBER, sep=&quot;:Stn&quot;), trawl = paste(station, &quot;trawl&quot;, sep=&quot;:&quot;)) Then we created the various layers of our Event Core. We created these layers/data frames from two separate datasets that data are pulled from - one dataset that contains the overall catch data, and one dataset that contains the specimen data: trawl2019_allCatch &lt;- read_excel(here(&quot;Trawl&quot;, &quot;2019&quot;, &quot;raw_data&quot;, &quot;2019_GoA_Fish_Trawl_catchdata.xlsx&quot;), sheet = &quot;CATCH_FINAL&quot;) %&gt;% mutate(project = &quot;IYS&quot;, cruise = paste(project, &quot;GoA2019&quot;, sep = &quot;:&quot;), station = paste(cruise, `TOW_NUMBER (number)`, sep = &quot;:Stn&quot;), trawl = paste(station, &quot;trawl&quot;, sep = &quot;:&quot;)) trawl2019_specimen &lt;- read_excel(here(&quot;Trawl&quot;, &quot;2019&quot;, &quot;raw_data&quot;, &quot;2019_GoA_Fish_Specimen_data.xlsx&quot;), sheet = &quot;SPECIMEN_FINAL&quot;) %&gt;% mutate(project = &quot;IYS&quot;, cruise = paste(project, &quot;GoA2019&quot;, sep = &quot;:&quot;), station = paste(cruise, TOW_NUMBER, sep = &quot;:Stn&quot;), trawl = paste(station, &quot;trawl&quot;, sep = &quot;:&quot;), sample = paste(trawl, &quot;sample&quot;, sep = &quot;:&quot;), sample = paste(sample, row_number(), sep = &quot;&quot;)) Next we created the Event Core, ensuring that we connect the data to the right layer (i.e. date and time should be connected to the layer associated with the sampling event). Please note that because we are creating multiple layers and nesting information, and then at a later stage combining different tables, this results in cells being populated with NA. These have to be removed prior to publishing the Event Core through the IPT. trawl2019_project &lt;- trawl2019 %&gt;% select(eventID = project) %&gt;% distinct(eventID) %&gt;% mutate(type = &quot;project&quot;) trawl2019_cruise &lt;- trawl2019 %&gt;% select(eventID = cruise, parentEventID = project) %&gt;% distinct(eventID, .keep_all = TRUE) %&gt;% mutate(type = &quot;cruise&quot;) trawl2019_station &lt;- trawl2019 %&gt;% select(eventID = station, parentEventID = cruise) %&gt;% distinct(eventID, .keep_all = TRUE) %&gt;% mutate(type = &quot;station&quot;) # The coordinates associated to the trawl need to be presented in a LINESTRING. # END_LONGITUDE_DD needs to be inverted (has to be between -180 and 180, inclusive). trawl2019_coordinates &lt;- trawl2019 %&gt;% select(eventID = trawl, START_LATITUDE_DD, longitude, END_LATITUDE_DD, END_LONGITUDE_DD) %&gt;% mutate(END_LONGITUDE_DD = END_LONGITUDE_DD * -1, footprintWKT = paste(&quot;LINESTRING (&quot;, longitude, START_LATITUDE_DD, &quot;,&quot;, END_LONGITUDE_DD, END_LATITUDE_DD, &quot;)&quot;)) trawl2019_linestring &lt;- obistools::calculate_centroid(trawl2019_coordinates$footprintWKT) trawl2019_linestring &lt;- cbind(trawl2019_coordinates, trawl2019_linestring) %&gt;% select(eventID, footprintWKT, decimalLatitude, decimalLongitude, coordinateUncertaintyInMeters) trawl2019_trawl &lt;- trawl2019 %&gt;% select(eventID = trawl, parentEventID = station, eventDate, year, month, day) %&gt;% mutate(minimumDepthInMeters = 0, # headrope was at the surface maximumDepthInMeters = trawl2019$MOUTH_OPENING_HEIGHT, samplingProtocol = &quot;midwater trawl&quot;, # when available add DOI to paper here locality = case_when( trawl2019$EVENT_SUB_TYPE == &quot;Can EEZ&quot; ~ &quot;Canadian EEZ&quot;), locationID = case_when( trawl2019$EVENT_SUB_TYPE == &quot;Can EEZ&quot; ~ &quot;http://marineregions.org/mrgid/8493&quot;)) %&gt;% left_join(trawl2019_linestring, by = &quot;eventID&quot;) %&gt;% distinct(eventID, .keep_all = TRUE) %&gt;% mutate(type = &quot;midwater trawl&quot;) trawl2019_sample &lt;- trawl2019_specimen %&gt;% select(eventID = sample, parentEventID = trawl) %&gt;% distinct(eventID, .keep_all = TRUE) %&gt;% mutate(type = &quot;individual sample&quot;) trawl2019_event &lt;- bind_rows(trawl2019_project, trawl2019_cruise, trawl2019_station, trawl2019_trawl, trawl2019_sample) # Remove NAs from the Event Core: trawl2019_event &lt;- sapply(trawl2019_event, as.character) trawl2019_event[is.na(trawl2019_event)] &lt;- &quot;&quot; trawl2019_event &lt;- as.data.frame(trawl2019_event) TO DO: Add visual of e.g. the top 10 rows of the Event Core. Now that we created the Event Core, we create the occurrence extension. To do this, we create two separate occurrence data tables: one that includes the occurrence data for the total catch, and one data table for the specimen data. Finally, the Occurrence extension is created by combining these two data frames. Personally, I prefer to re-order it so it makes visual sense to me (nest the specimen occurrence records under their respective overall catch data). trawl2019_allCatch_worms &lt;- worrms::wm_records_names(unique(trawl2019_allCatch$scientificname)) trawl2019_occ &lt;- left_join(trawl2019_allCatch, trawl2019_allCatch_worms, by = &quot;scientificname&quot;) %&gt;% rename(eventID = trawl, specificEpithet = species, scientificNameAuthorship = authority, taxonomicStatus = status, taxonRank = rank, scientificName = scientificname, scientificNameID = lsid, individualCount = `CATCH_COUNT (pieces)(**includes Russian expansion for some species)`, occurrenceRemarks = COMMENTS) %&gt;% mutate(occurrenceID = paste(eventID, &quot;occ&quot;, sep = &quot;:&quot;), occurrenceID = paste(occurrenceID, row_number(), sep = &quot;:&quot;), occurrenceStatus = &quot;present&quot;, sex = &quot;&quot;) trawl2019_catch_ind_worms &lt;- worrms::wm_records_names(unique(trawl2019_catch_ind$scientificname)) %&gt;% bind_rows() trawl2019_catch_ind_occ &lt;- left_join(trawl2019_catch_ind, trawl2019_catch_ind_worms, by = &quot;scientificname&quot;) %&gt;% rename(scientificNameAuthorship = authority, taxonomicStatus = status, taxonRank = rank, scientificName = scientificname, scientificNameID = lsid) %&gt;% mutate(occurrenceID = paste(eventID, &quot;occ&quot;, sep = &quot;:&quot;), occurrenceStatus = &quot;present&quot;, individualCount = 1) # Combine the two occurrence data frames: trawl2019_occ_ext &lt;- dplyr::bind_rows(trawl2019_occ_fnl, trawl2019_catch_ind_fnl) # To re-order the occurrenceID, use following code: order &lt;- stringr::str_sort(trawl2019_occ_ext$occurrenceID, numeric=TRUE) trawl2019_occ_ext &lt;- trawl2019_occ_ext[match(order, trawl2019_occ_ext$occurrenceID),] %&gt;% mutate(basisOfRecord = &quot;HumanObservation&quot;) TO DO: Add visual of e.g. the top 10 rows of the Occurrence extension. Please note that in the overall species-specific occurrence data frame, individualCount was not included. This term should not be used for abundance studies, but to avoid confusion and the appearance that the specimen records are an additional observation on top of the overall catch record, the individualCount term was left blank for the overall catch data. A resource relationship extension is created to further highlight that the individual samples in the occurrence extension are part of a larger overall catch that was also listed in the occurrence extension. In this extension, we wanted to make sure to highlight that the specimen occurrence records are a subset of the overall catch data through the field relationshipOfResource1. Each of these relationships gets a unique resourceRelationshipID. trawl_resourceRelationship &lt;- trawl2019_occ_ext %&gt;% select(eventID, occurrenceID, scientificName) %&gt;% mutate(resourceID = ifelse(grepl(&quot;sample&quot;, trawl2019_occ_ext$occurrenceID), trawl2019_occ_ext$occurrenceID, NA)) %&gt;% mutate(eventID = gsub(&quot;:sample.*&quot;, &quot;&quot;, trawl2019_occ_ext$eventID)) %&gt;% group_by(eventID, scientificName) %&gt;% filter(n() != 1) %&gt;% ungroup() trawl_resourceRelationship &lt;- trawl_resourceRelationship %&gt;% mutate(relatedResourceID = ifelse(grepl(&quot;sample&quot;, trawl_resourceRelationship$occurrenceID), NA, trawl_resourceRelationship$occurrenceID)) %&gt;% mutate(relationshipOfResource = ifelse(!is.na(resourceID), &quot;is a subset of&quot;, NA)) %&gt;% dplyr::arrange(eventID, scientificName) %&gt;% fill(relatedResourceID) %&gt;% filter(!is.na(resourceID)) order &lt;- stringr::str_sort(trawl_resourceRelationship$resourceID, numeric = TRUE) trawl_resourceRelationship &lt;- trawl_resourceRelationship[match(order, trawl_resourceRelationship$resourceID),] trawl_resourceRelationship &lt;- trawl_resourceRelationship %&gt;% mutate(resourceRelationshipID = paste(relatedResourceID, &quot;rr&quot;, sep = &quot;:&quot;), ID = sprintf(&quot;%03d&quot;, row_number()), resourceRelationshipID = paste(resourceRelationshipID, ID, sep = &quot;:&quot;)) %&gt;% select(eventID, resourceRelationshipID, resourceID, relationshipOfResource, relatedResourceID) TO DO: Add visual of e.g. the top 10 rows of the ResourceRelationship extension. 2.3.2 FAQ Q1. Why not use the terms associatedOccurrence or associatedTaxa? A. There seems to be a movement away from the term associatedOccurrence as the resourceRelationship extension has a much broader use case. Some issues that were raised on GitHub exemplify this, see e.g. here. associatedTaxa is used to provide identifiers or names of taxa and the associations of an Occurrence with them. This term is not apt for establishing relationships between taxa, only between specific Occurrences of an organism with other taxa. As noted on the TDWG website, […] Note that the ResourceRelationship class is an alternative means of representing associations, and with more detail. See also e.g. this issue. 2.4 Aligning Data to Darwin Core - Sampling Event with Measurement or Fact Abby Benson October 8,2019 2.4.1 General information about this notebook This notebook was created for the IOOS DMAC Code Sprint Biological Data Session The data in this notebook were created specifically as an example and meant solely to be illustrative of the process for aligning data to the biological data standard - Darwin Core. These data should not be considered actually occurrences of species and any measurements are also contrived. This notebook is meant to provide a step by step process for taking original data and aligning it to Darwin Core. library(readr) library(uuid) library(dplyr) MadeUpDataForBiologicalDataTraining &lt;- read_csv(&quot;~/OBIS/Reference Documentation/Presentations/IOOS DMAC Code Sprint/MadeUpDataForBiologicalDataTraining.csv&quot;) ## Parsed with column specification: ## cols( ## date = col_character(), ## lat = col_double(), ## lon = col_double(), ## region = col_character(), ## station = col_double(), ## transect = col_double(), ## `scientific name` = col_character(), ## `percent cover` = col_double(), ## depth = col_double(), ## `bottom type` = col_character(), ## rugosity = col_double(), ## temperature = col_double() ## ) First we need to to decide if we will provide an occurrence only version of the data or a sampling event with measurement or facts version of the data. Occurrence only is easier to create. It’s only one file to produce. However, several pieces of information will be left out if we choose that option. If we choose to do sampling event with measurement or fact we’ll be able to capture all of the data in the file creating a lossless version. Here we decide to use the sampling event option to include as much information as we can. First let’s create the eventID and occurrenceID in the original file so that information can be reused for all necessary files down the line. MadeUpDataForBiologicalDataTraining$eventID &lt;- paste(MadeUpDataForBiologicalDataTraining$region, MadeUpDataForBiologicalDataTraining$station, MadeUpDataForBiologicalDataTraining$transect, sep = &quot;_&quot;) MadeUpDataForBiologicalDataTraining$occurrenceID &lt;- &quot;&quot; MadeUpDataForBiologicalDataTraining$occurrenceID &lt;- sapply(MadeUpDataForBiologicalDataTraining$occurrenceID, function(x) UUIDgenerate(use.time = TRUE)) We will need to create three separate files to comply with the sampling event format. We’ll start with the event file but we only need to include the columns that are relevant to the event file. event &lt;- MadeUpDataForBiologicalDataTraining[c(&quot;date&quot;, &quot;lat&quot;, &quot;lon&quot;, &quot;region&quot;, &quot;station&quot;, &quot;transect&quot;, &quot;depth&quot;, &quot;bottom type&quot;, &quot;eventID&quot;)] Next we need to rename any columns of data that match directly to Darwin Core. We know this based on our crosswalk spreadsheet CrosswalkToDarwinCore.csv event$decimalLatitude &lt;- event$lat event$decimalLongitude &lt;- event$lon event$minimumDepthInMeters &lt;- event$depth event$maximumDepthInMeters &lt;- event$depth event$habitat &lt;- event$`bottom type` event$island &lt;- event$region Let’s see how it looks: head(event, n = 10) ## # A tibble: 10 x 15 ## date lat lon region station transect depth `bottom type` eventID ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 7/16~ 18.3 -64.8 St. J~ 250 1 25 shallow reef~ St. Jo~ ## 2 7/16~ 18.3 -64.8 St. J~ 250 1 25 shallow reef~ St. Jo~ ## 3 7/16~ 18.3 -64.8 St. J~ 250 1 25 shallow reef~ St. Jo~ ## 4 7/16~ 18.3 -64.8 St. J~ 250 1 25 shallow reef~ St. Jo~ ## 5 7/16~ 18.3 -64.8 St. J~ 250 2 35 complex back~ St. Jo~ ## 6 7/16~ 18.3 -64.8 St. J~ 250 2 35 complex back~ St. Jo~ ## 7 7/16~ 18.3 -64.8 St. J~ 250 2 35 complex back~ St. Jo~ ## 8 7/16~ 18.3 -64.8 St. J~ 250 2 35 complex back~ St. Jo~ ## 9 7/16~ 18.3 -64.8 St. J~ 250 3 85 deep reef St. Jo~ ## 10 7/16~ 18.3 -64.8 St. J~ 250 3 85 deep reef St. Jo~ ## # ... with 6 more variables: decimalLatitude &lt;dbl&gt;, ## # decimalLongitude &lt;dbl&gt;, minimumDepthInMeters &lt;dbl&gt;, ## # maximumDepthInMeters &lt;dbl&gt;, habitat &lt;chr&gt;, island &lt;chr&gt; We need to convert the date to ISO format event$eventDate &lt;- as.Date(event$date, format = &quot;%m/%d/%Y&quot;) We will also have to add any missing required fields event$basisOfRecord &lt;- &quot;HumanObservation&quot; event$geodeticDatum &lt;- &quot;EPSG:4326 WGS84&quot; Then we’ll remove any columns that we no longer need to clean things up a bit. event$date &lt;- NULL event$lat &lt;- NULL event$lon &lt;- NULL event$region &lt;- NULL event$station &lt;- NULL event$transect &lt;- NULL event$depth &lt;- NULL event$`bottom type` &lt;- NULL We have too many repeating rows of information. We can pare this down using eventID which is a unique identifier for each sampling event in the data- which is six, three transects per site. event &lt;- event[which(!duplicated(event$eventID)),] head(event, n = 6) ## # A tibble: 6 x 10 ## eventID decimalLatitude decimalLongitude minimumDepthInM~ ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 St. Jo~ 18.3 -64.8 25 ## 2 St. Jo~ 18.3 -64.8 35 ## 3 St. Jo~ 18.3 -64.8 85 ## 4 St. Jo~ 18.3 -64.8 28 ## 5 St. Jo~ 18.3 -64.8 16 ## 6 St. Jo~ 18.3 -64.8 90 ## # ... with 6 more variables: maximumDepthInMeters &lt;dbl&gt;, habitat &lt;chr&gt;, ## # island &lt;chr&gt;, eventDate &lt;date&gt;, basisOfRecord &lt;chr&gt;, ## # geodeticDatum &lt;chr&gt; Finally we write out the event file write.csv(event, file=&quot;MadeUpData_event.csv&quot;, row.names=FALSE, fileEncoding=&quot;UTF-8&quot;, quote=TRUE) Next we need to create the occurrence file. We start by creating the dataframe. occurrence &lt;- MadeUpDataForBiologicalDataTraining[c(&quot;scientific name&quot;, &quot;eventID&quot;, &quot;occurrenceID&quot;, &quot;percent cover&quot;)] Then we’ll rename the columns that align directly with Darwin Core. occurrence$scientificName &lt;- occurrence$`scientific name` Finally we’ll add required information that’s missing. occurrence$occurrenceStatus &lt;- ifelse (occurrence$`percent cover` == 0, &quot;absent&quot;, &quot;present&quot;) 2.4.2 Taxonomic Name Matching A requirement for OBIS is that all scientific names match to the World Register of Marine Species (WoRMS) and a scientificNameID is included. A scientificNameID looks like this “urn:lsid:marinespecies.org:taxname:275730” with the last digits after the colon being the WoRMS aphia ID. We’ll need to go out to WoRMS to grab this information. Create a lookup table of unique scientific names lut_worms &lt;- as.data.frame(unique(occurrence_only$scientificName)) lut_worms$scientificName &lt;- as.character(lut_worms$`unique(occurrence_only$scientificName)`) lut_worms$`unique(occurrence_only$scientificName)` &lt;- NULL lut_worms$scientificName &lt;- as.character(lut_worms$scientificName) Add the columns that we can grab information from WoRMS including the required scientificNameID. lut_worms$acceptedname &lt;- &quot;&quot; lut_worms$acceptedID &lt;- &quot;&quot; lut_worms$scientificNameID &lt;- &quot;&quot; lut_worms$kingdom &lt;- &quot;&quot; lut_worms$phylum &lt;- &quot;&quot; lut_worms$class &lt;- &quot;&quot; lut_worms$order &lt;- &quot;&quot; lut_worms$family &lt;- &quot;&quot; lut_worms$genus &lt;- &quot;&quot; lut_worms$scientificNameAuthorship &lt;- &quot;&quot; lut_worms$taxonRank &lt;- &quot;&quot; Taxonomic lookup using the library taxizesoap for (i in 1:nrow(lut_worms)){ df &lt;- worms_records(scientific = lut_worms$scientificName[i]) lut_worms[i,]$scientificNameID &lt;- df$lsid[1] lut_worms[i,]$acceptedname &lt;- df$valid_name[1] lut_worms[i,]$acceptedID &lt;- df$valid_AphiaID[1] lut_worms[i,]$kingdom &lt;- df$kingdom[1] lut_worms[i,]$phylum &lt;- df$phylum[1] lut_worms[i,]$class &lt;- df$class[1] lut_worms[i,]$order &lt;- df$order[1] lut_worms[i,]$family &lt;- df$family[1] lut_worms[i,]$genus &lt;- df$genus[1] lut_worms[i,]$scientificNameAuthorship &lt;- df$authority[1] lut_worms[i,]$taxonRank &lt;- df$rank[1] message(paste(&quot;Looking up information for species:&quot;, lut_worms[i,]$scientificName)) } ## Looking up information for species: Acropora cervicornis ## Looking up information for species: Madracis auretenra ## Looking up information for species: Mussa angulosa ## Looking up information for species: Siderastrea radians Merge the lookup table of unique scientific names back with the occurrence data. occurrence &lt;- merge(occurrence, lut_worms, by = &quot;scientificName&quot;) We’re going to remove any unnecessary columns to clean up the file occurrence$`scientific name` &lt;- NULL occurrence$`percent cover` &lt;- NULL Quick look at what we have before we write out the file head(occurrence, n = 10) ## scientificName eventID ## 1 Acropora cervicornis St. John_250_1 ## 2 Acropora cervicornis St. John_250_2 ## 3 Acropora cervicornis St. John_250_3 ## 4 Acropora cervicornis St. John_356_1 ## 5 Acropora cervicornis St. John_356_2 ## 6 Acropora cervicornis St. John_356_3 ## 7 Madracis auretenra St. John_250_1 ## 8 Madracis auretenra St. John_250_2 ## 9 Madracis auretenra St. John_250_3 ## 10 Madracis auretenra St. John_356_1 ## occurrenceID occurrenceStatus ## 1 63be8f7e-ea9a-11e9-8649-49c6324f3a06 absent ## 2 63beb687-ea9a-11e9-8649-49c6324f3a06 absent ## 3 63beb68b-ea9a-11e9-8649-49c6324f3a06 present ## 4 63bedd76-ea9a-11e9-8649-49c6324f3a06 present ## 5 63bedd7a-ea9a-11e9-8649-49c6324f3a06 present ## 6 63bedd7e-ea9a-11e9-8649-49c6324f3a06 present ## 7 63beb684-ea9a-11e9-8649-49c6324f3a06 present ## 8 63beb688-ea9a-11e9-8649-49c6324f3a06 present ## 9 63beb68c-ea9a-11e9-8649-49c6324f3a06 absent ## 10 63bedd77-ea9a-11e9-8649-49c6324f3a06 present ## acceptedname acceptedID ## 1 Acropora cervicornis 206989 ## 2 Acropora cervicornis 206989 ## 3 Acropora cervicornis 206989 ## 4 Acropora cervicornis 206989 ## 5 Acropora cervicornis 206989 ## 6 Acropora cervicornis 206989 ## 7 Madracis auretenra 430664 ## 8 Madracis auretenra 430664 ## 9 Madracis auretenra 430664 ## 10 Madracis auretenra 430664 ## scientificNameID kingdom phylum class ## 1 urn:lsid:marinespecies.org:taxname:206989 Animalia Cnidaria Anthozoa ## 2 urn:lsid:marinespecies.org:taxname:206989 Animalia Cnidaria Anthozoa ## 3 urn:lsid:marinespecies.org:taxname:206989 Animalia Cnidaria Anthozoa ## 4 urn:lsid:marinespecies.org:taxname:206989 Animalia Cnidaria Anthozoa ## 5 urn:lsid:marinespecies.org:taxname:206989 Animalia Cnidaria Anthozoa ## 6 urn:lsid:marinespecies.org:taxname:206989 Animalia Cnidaria Anthozoa ## 7 urn:lsid:marinespecies.org:taxname:430664 Animalia Cnidaria Anthozoa ## 8 urn:lsid:marinespecies.org:taxname:430664 Animalia Cnidaria Anthozoa ## 9 urn:lsid:marinespecies.org:taxname:430664 Animalia Cnidaria Anthozoa ## 10 urn:lsid:marinespecies.org:taxname:430664 Animalia Cnidaria Anthozoa ## order family genus scientificNameAuthorship ## 1 Scleractinia Acroporidae Acropora (Lamarck, 1816) ## 2 Scleractinia Acroporidae Acropora (Lamarck, 1816) ## 3 Scleractinia Acroporidae Acropora (Lamarck, 1816) ## 4 Scleractinia Acroporidae Acropora (Lamarck, 1816) ## 5 Scleractinia Acroporidae Acropora (Lamarck, 1816) ## 6 Scleractinia Acroporidae Acropora (Lamarck, 1816) ## 7 Scleractinia Pocilloporidae Madracis Locke, Weil &amp; Coates, 2007 ## 8 Scleractinia Pocilloporidae Madracis Locke, Weil &amp; Coates, 2007 ## 9 Scleractinia Pocilloporidae Madracis Locke, Weil &amp; Coates, 2007 ## 10 Scleractinia Pocilloporidae Madracis Locke, Weil &amp; Coates, 2007 ## taxonRank ## 1 Species ## 2 Species ## 3 Species ## 4 Species ## 5 Species ## 6 Species ## 7 Species ## 8 Species ## 9 Species ## 10 Species Write out the file. All done with occurrence! write.csv(occurrence, file=&quot;MadeUpData_Occurrence.csv&quot;, row.names=FALSE, fileEncoding=&quot;UTF-8&quot;, quote=TRUE) The last file we need to create is the measurement or fact file. For this we need to combine all of the measurements or facts that we want to include making sure to include IDs from the BODC NERC vocabulary where possible. temperature &lt;- MadeUpDataForBiologicalDataTraining[c(&quot;eventID&quot;, &quot;temperature&quot;, &quot;date&quot;)] temperature$occurrenceID &lt;- &quot;&quot; temperature$measurementType &lt;- &quot;temperature&quot; temperature$measurementTypeID &lt;- &quot;http://vocab.nerc.ac.uk/collection/P25/current/WTEMP/&quot; temperature$measurementValue &lt;- temperature$temperature temperature$measurementUnit &lt;- &quot;Celsius&quot; temperature$measurementUnitID &lt;- &quot;http://vocab.nerc.ac.uk/collection/P06/current/UPAA/&quot; temperature$measurementAccuracy &lt;- 3 temperature$measurementDeterminedDate &lt;- as.Date(temperature$date, format = &quot;%m/%d/%Y&quot;) temperature$measurementMethod &lt;- &quot;&quot; temperature$temperature &lt;- NULL temperature$date &lt;- NULL rugosity &lt;- MadeUpDataForBiologicalDataTraining[c(&quot;eventID&quot;, &quot;rugosity&quot;, &quot;date&quot;)] rugosity$occurrenceID &lt;- &quot;&quot; rugosity$measurementType &lt;- &quot;rugosity&quot; rugosity$measurementTypeID &lt;- &quot;&quot; rugosity$measurementValue &lt;- rugosity$rugosity rugosity$measurementUnit &lt;- &quot;&quot; rugosity$measurementUnitID &lt;- &quot;&quot; rugosity$measuremntAccuracy &lt;- &quot;&quot; rugosity$measurementDeterminedDate &lt;- as.Date(rugosity$date, format = &quot;%m/%d/%Y&quot;) rugosity$measurementMethod &lt;- &quot;&quot; rugosity$rugosity &lt;- NULL rugosity$date &lt;- NULL percentcover &lt;- MadeUpDataForBiologicalDataTraining[c(&quot;eventID&quot;, &quot;occurrenceID&quot;, &quot;percent cover&quot;, &quot;date&quot;)] percentcover$measurementType &lt;- &quot;Percent Cover&quot; percentcover$measurementTypeID &lt;- &quot;http://vocab.nerc.ac.uk/collection/P01/current/SDBIOL10/&quot; percentcover$measurementValue &lt;- percentcover$`percent cover` percentcover$measurementUnit &lt;- &quot;Percent/100m^2&quot; percentcover$measurementUnitID &lt;- &quot;&quot; percentcover$measuremntAccuracy &lt;- 5 percentcover$measurementDeterminedDate &lt;- as.Date(percentcover$date, format = &quot;%m/%d/%Y&quot;) percentcover$measurementMethod &lt;- &quot;&quot; percentcover$`percent cover` &lt;- NULL percentcover$date &lt;- NULL measurementOrFact &lt;- rbind(temperature, rugosity, percentcover) ## Error in match.names(clabs, names(xi)): names do not match previous names Let’s check to see what it looks like head(measurementOrFact, n = 50) ## # A tibble: 50 x 9 ## eventID occurrenceID measurementType measurementType~ measurementValue ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 St. Jo~ &quot;&quot; temperature &quot;&quot; 25.2 ## 2 St. Jo~ &quot;&quot; temperature &quot;&quot; 25.2 ## 3 St. Jo~ &quot;&quot; temperature &quot;&quot; 25.2 ## 4 St. Jo~ &quot;&quot; temperature &quot;&quot; 25.2 ## 5 St. Jo~ &quot;&quot; temperature &quot;&quot; 24.8 ## 6 St. Jo~ &quot;&quot; temperature &quot;&quot; 24.8 ## 7 St. Jo~ &quot;&quot; temperature &quot;&quot; 24.8 ## 8 St. Jo~ &quot;&quot; temperature &quot;&quot; 24.8 ## 9 St. Jo~ &quot;&quot; temperature &quot;&quot; 23.1 ## 10 St. Jo~ &quot;&quot; temperature &quot;&quot; 23.1 ## # ... with 40 more rows, and 4 more variables: measurementUnit &lt;chr&gt;, ## # measurementUnitID &lt;chr&gt;, measurementDeterminedDate &lt;date&gt;, ## # measurementMethod &lt;chr&gt; write.csv(measurementOrFact, file=&quot;MadeUpData_mof.csv&quot;, row.names=FALSE, fileEncoding=&quot;UTF-8&quot;, quote=TRUE) "],["dealing-with-errors.html", "Chapter 3 Dealing with errors 3.1 Example using GitHub to resolve errors", " Chapter 3 Dealing with errors Datasets can have a wide variety of errors that pop up during the darwin core alignment process. This chapter details ways in which a data manager can identify, discuss, and resolve potential errors in the data. It should be noted that, in most cases, the data manager/scientist aligning the data to darwin core should reach out to the data originator to ensure the actions taken are not incorrectly representing the observations. 3.1 Example using GitHub to resolve errors Dataset sent to OBIS-USA via email. OBIS-USA uploaded to IPT. Once the data were uploaded, the IPT identified there was an issue with the occurrenceID field. The issue was then presented and discussed in a GitHub ticket: The data manager uploaded the raw data and code to GitHub through the pull request below. This included a fix for the occurrenceID issue. The OBIS node manager was notified of the availability of a revised dataset by pointing directly to the appropriate commit in GitHub: The OBIS node manager downloaded the data from the commit above and uploaded them to the IPT. The IPT returned a summary of the dataset including that 434 records had invalid scientificNameID records in the occurrence file. After some data sleuthing, the data manager noticed that the code accidentally removed trailing zeros from scientificNameID that ended in 0: So, the data manager updated the code to resolve the issue and generate a new occurrence file. Here is fixing the scientificNameID generation: Here is removing the problematic code: The revised occurrence file was then resubmitted to the OBIS node manager by pointing them at the appropriate commit record: The OBIS node manager downloaded the data from the commit above and uploaded them to the IPT. The IPT and OBIS landing page now indicated that no more issues with these data are present: "],["frequently-asked-questions.html", "Chapter 4 Frequently Asked Questions", " Chapter 4 Frequently Asked Questions Q. What data structure does OBIS recommend? A. The OBIS-ENV Darwin Core Archive Data Structure. OBIS manual Q. What is a controlled vocabulary, why use them? A. There are a number of controlled vocabularies that are used to describe parameters commonly used in specific research domains. Using terms defined in a controlled vocabulary allows for greater interoperability of data sets within the domain, and ideally between domains by ensuring that variables that are the same can be identified. Q. What controlled vocabularies does OBIS rely on? A. WoRMS, NERC Vocabulary Server inlcuding: * Device categories using the SeaDataNet device categories Device make/model using the SeaVoX Device Catalogue Platform categories using SeaVoX Platform Categories Platform instances using the ICES Platform Codes Unit of measure Q. How can I find out which common measurementTypes are used in measurement or facts tables in existing OBIS datasets? A. See Measurement Types in OBIS Q. What is an ontology? A. An ontology is a classification system for establishing a hierarchically related set of concepts. Concepts are often terms from controlled vocabularies. Ontologies can include all of the following, but are not required to include them. Classes (general things, types of things) Instances (individual things) Relationships among things Properties of things Functions, processes, constraints, and rules relating to things Q. What is ERDDAP? A. ERDDAP is a data server. It provides ‘easier access to scientific data’ by providing a consistent interface that aggregates many disparate data sources. It does this by providing translation services between many common file types for gridded arrarys (‘net CDF’ files) and tabular data (spreadsheets). Data access is also made easier because it unifies different types of data servers and access protocols. Q. What metadata profile does OBIS use? A. OBIS uses the GBIF EML profile (version 1.1) Q. Can Darwin Core be used in the Semantic Web/Resrouce Description Framework? A. See Darwin Core Resource Description Framework Guide and Lessons learned from adapting the Darwin Core vocabulary standard for use in RDF "],["tools.html", "Chapter 5 Tools 5.1 R 5.2 Python 5.3 Google Sheets 5.4 Validators", " Chapter 5 Tools Below are some of the tools and packages used in workflows. R and Python package “Type” is BIO for packages specifically for biological applications, and GEN for generic packages. 5.1 R Package Type Description bdveRse BIO A family of R packages for biodiversity data. ecocomDP BIO Work with the Ecological Community Data Design Pattern. ‘ecocomDP’ is a flexible data model for harmonizing ecological community surveys, in a research question agnostic format, from source data published across repositories, and with methods that keep the derived data up-to-date as the underlying sources change. EDIorg/EMLasseblyline BIO For scientists and data managers to create high quality EML metadata for dataset publication. finch BIO Parse Darwin Core Files iobis/obistools BIO Tools for data enhancement and quality control. robis BIO R client for the OBIS API ropensci/EML BIO Provides support for the serializing and parsing of all low-level EML concepts taxize BIO Interacts with a suite of web ‘APIs’ for taxonomic tasks, such as getting database specific taxonomic identifiers, verifying species names, getting taxonomic hierarchies, fetching downstream and upstream taxonomic names, getting taxonomic synonyms, converting scientific to common names and vice versa, and more. worrms BIO Client for World Register of Marine Species. Includes functions for each of the API methods, including searching for names by name, date and common names, searching using external identifiers, fetching synonyms, as well as fetching taxonomic children and taxonomic classification. Hmisc GEN Contains many functions useful for data analysis, high-level graphics, utility operations, functions for computing sample size and power, simulation, importing and annotating datasets, imputing missing values, advanced table making, variable clustering, character string manipulation, conversion of R objects to LaTeX and html code, and recoding variables. Particularly check out the describe() function. lubridate GEN Functions to work with date-times and time-spans: fast and user friendly parsing of date-time data, extraction and updating of components of a date-time (years, months, days, hours, minutes, and seconds), algebraic manipulation on date-time and time-span objects. stringr GEN Simple, Consistent Wrappers for Common String Operations tidyverse GEN The ‘tidyverse’ is a set of packages that work in harmony because they share common data representations and ‘API’ design. This package is designed to make it easy to install and load multiple ‘tidyverse’ packages in a single step. uuid GEN Tools for generating and handling of UUIDs (Universally Unique Identifiers). 5.2 Python Package Type Description metapype BIO A lightweight Python 3 library for generating EML metadata python-dwca-reader BIO A simple Python package to read and parse Darwin Core Archive (DwC-A) files, as produced by the GBIF website, the IPT and many other biodiversity informatics tools. pyworms BIO Python client for the World Register of Marine Species (WoRMS) REST service. numpy GEN NumPy (Numerical Python) is an open source Python library that’s used in almost every field of science and engineering. It’s the universal standard for working with numerical data in Python, and it’s at the core of the scientific Python and PyData ecosystems. pandas GEN pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. Super helpful when manipulating tabular data! uuid GEN This module provides immutable UUID objects (class UUID) and the functions uuid1(), uuid3(), uuid4(), uuid5() for generating version 1, 3, 4, and 5 UUIDs as specified in RFC 4122. Built in – part of the Python standard library. obis-qc BIO Quality checks on occurrence records. Checks occurrenceStatus, individualCount, eventDate, decimalLatitude, decimalLongitude, coordinateUncertaintyInMeters, minimumDepthInMeters, maximumDepthInMeters, scientificName, scientificNameID. Checks from Vandepitte et al. flags not implemented: 3, 9, 14, 15, 16, 10, 17, 21-30. biopython BIO Biopython is a set of freely available tools for biological computation written in Python by an international team of developers. It is a distributed collaborative effort to develop Python libraries and applications which address the needs of current and future work in bioinformatics. 5.3 Google Sheets Package Description Google Sheet DarwinCore Archive Assistant add-on Google Sheet add-on which assists the creation of Darwin Core Archives (DwCA) and publising to Zenodo. DwCA’s are stored into user’s Google Drive and can be downloaded for upload into IPT installations or other software which is able to read DwC-archives. 5.4 Validators Name Description Darwin Core Archive Validator This validator verifies the structural integrity of a Darwin Core Archive. It does not check the data values, such as coordinates, dates or scientific names. GBIF DATA VALIDATOR The GBIF data validator is a service that allows anyone with a GBIF-relevant dataset to receive a report on the syntactical correctness and the validity of the content contained within the dataset. LifeWatch Belgium Through this interactive section of the LifeWatch.be portal users can upload their own data using a standard data format, and choose from several web services, models and applications to process the data. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
